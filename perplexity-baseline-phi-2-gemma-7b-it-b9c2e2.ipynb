{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":67121,"databundleVersionId":7806901,"sourceType":"competition"},{"sourceId":7747717,"sourceType":"datasetVersion","datasetId":4506214},{"sourceId":164836055,"sourceType":"kernelVersion"},{"sourceId":4298,"sourceType":"modelInstanceVersion","modelInstanceId":3093},{"sourceId":4302,"sourceType":"modelInstanceVersion","modelInstanceId":3097},{"sourceId":5112,"sourceType":"modelInstanceVersion","modelInstanceId":3900},{"sourceId":5994,"sourceType":"modelInstanceVersion","modelInstanceId":4761},{"sourceId":10716,"sourceType":"modelInstanceVersion","modelInstanceId":8658},{"sourceId":11261,"sourceType":"modelInstanceVersion","modelInstanceId":8332},{"sourceId":11382,"sourceType":"modelInstanceVersion","modelInstanceId":8318}],"dockerImageVersionId":30665,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":194.190402,"end_time":"2024-03-01T09:46:19.293664","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-03-01T09:43:05.103262","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"00d5e991d038489a8faab01b509dc81f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"136bdc133f3647e299cc586f88d80d2e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1f8b548c1b9d447bb2cdb7f1e0349571":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1ff1633c121840048eec0a2fb7daf9da":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"253748ba254a4f999a8037fd15e1587b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2c21a0ceb8c24bfa84511fdd0f258d38":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_349bbcac49e44b9ea69237def7e00a70","IPY_MODEL_553bb142f5bf400089a954065a2e61e8","IPY_MODEL_d2862781640749cd8f4b6ce0d4747b1a"],"layout":"IPY_MODEL_5a449a57b1744b228b16ba3e9a787c60"}},"31770f23e0f84b1ebf68ddfa1a280ba6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_618ef1faa2f0431aa961ab08ec2ac105","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b81b5104c16d471b9f40f705a493ee57","value":2}},"349bbcac49e44b9ea69237def7e00a70":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_50d2fe6d91b5468f80dda3f49feb2eb4","placeholder":"​","style":"IPY_MODEL_253748ba254a4f999a8037fd15e1587b","value":"100%"}},"50d2fe6d91b5468f80dda3f49feb2eb4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"553bb142f5bf400089a954065a2e61e8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b1b275072dc844018b913a0f91b76113","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_00d5e991d038489a8faab01b509dc81f","value":1}},"5a449a57b1744b228b16ba3e9a787c60":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ff8737e614a4a6fa5b3ee1c6853ba01":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"618ef1faa2f0431aa961ab08ec2ac105":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c3b24c5949c4ee387964c48ab63b353":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b1b275072dc844018b913a0f91b76113":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b81b5104c16d471b9f40f705a493ee57":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d2862781640749cd8f4b6ce0d4747b1a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_edc853b1c9d545b5ab46f1b3c82be681","placeholder":"​","style":"IPY_MODEL_1f8b548c1b9d447bb2cdb7f1e0349571","value":" 1/1 [00:03&lt;00:00,  3.89s/it]"}},"d6282317e5c64154b5ffb244974e3c4c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ff1633c121840048eec0a2fb7daf9da","placeholder":"​","style":"IPY_MODEL_6c3b24c5949c4ee387964c48ab63b353","value":"Loading checkpoint shards: 100%"}},"d6fb5f723e38457fb670cb3afae98ab6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d6282317e5c64154b5ffb244974e3c4c","IPY_MODEL_31770f23e0f84b1ebf68ddfa1a280ba6","IPY_MODEL_fd0d7a4e7e1b4d30904bc865e838cb19"],"layout":"IPY_MODEL_ed5727d8ff4942eba79e43de5e355e9c"}},"ed5727d8ff4942eba79e43de5e355e9c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"edc853b1c9d545b5ab46f1b3c82be681":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd0d7a4e7e1b4d30904bc865e838cb19":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ff8737e614a4a6fa5b3ee1c6853ba01","placeholder":"​","style":"IPY_MODEL_136bdc133f3647e299cc586f88d80d2e","value":" 2/2 [01:46&lt;00:00, 49.39s/it]"}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n# If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed\n!pip install --no-index /kaggle/input/making-wheels-of-necessary-packages-for-hf-llms/bitsandbytes-0.42.0-py3-none-any.whl --find-links=/kaggle/input/making-wheels-of-necessary-packages-for-hf-llms\n!pip install --no-index /kaggle/input/making-wheels-of-necessary-packages-for-hf-llms/accelerate-0.27.2-py3-none-any.whl --find-links=/kaggle/input/making-wheels-of-necessary-packages-for-hf-llms\n!pip install --no-index /kaggle/input/making-wheels-of-necessary-packages-for-hf-llms/transformers-4.38.1-py3-none-any.whl --find-links=/kaggle/input/making-wheels-of-necessary-packages-for-hf-llms\n!pip install --no-index /kaggle/input/making-wheels-of-necessary-packages-for-hf-llms/optimum-1.17.1-py3-none-any.whl --find-links=/kaggle/input/making-wheels-of-necessary-packages-for-hf-llms","metadata":{"papermill":{"duration":54.916984,"end_time":"2024-03-01T09:44:02.702691","exception":false,"start_time":"2024-03-01T09:43:07.785707","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-05T06:25:54.341997Z","iopub.execute_input":"2024-04-05T06:25:54.342346Z","iopub.status.idle":"2024-04-05T06:26:51.002008Z","shell.execute_reply.started":"2024-04-05T06:25:54.342313Z","shell.execute_reply":"2024-04-05T06:26:51.000735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import bitsandbytes\nimport accelerate\nimport transformers\nimport optimum","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"papermill":{"duration":6.461738,"end_time":"2024-03-01T09:44:09.16901","exception":false,"start_time":"2024-03-01T09:44:02.707272","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-05T06:26:51.003417Z","iopub.execute_input":"2024-04-05T06:26:51.003701Z","iopub.status.idle":"2024-04-05T06:26:56.927589Z","shell.execute_reply.started":"2024-04-05T06:26:51.003675Z","shell.execute_reply":"2024-04-05T06:26:56.926623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## bitsandbytes:\n\nЦя бібліотека використовується для оптимізації тренування глибоких нейронних мереж, зокрема забезпечуючи більш ефективні варіанти виконання операцій на низькому рівні, таких як робота з вагами моделі у форматі з меншою точністю (наприклад, 8-бітне квантування). Це може покращити швидкість навчання та зменшити вимоги до пам'яті без значної втрати точності.\n## accelerate\nРозроблений командою Hugging Face, цей пакет допомагає легко переносити код, який працює на одному пристрої, на багато GPU або навіть TPU, без потреби в розробці спеціалізованого коду для паралельного виконання. Він спрощує масштабування тренування моделей на різних обчислювальних платформах.\n## transformers\nЩе одна бібліотека від Hugging Face, яка забезпечує доступ до великої кількості переднавчених моделей на базі трансформерів (як BERT, GPT), що можна використовувати для різноманітних завдань з обробки природної мови (NLP), таких як класифікація текстів, генерація текстів, аналіз сентименту тощо.\n## optimum\nЦей пакет також розроблений Hugging Face і призначений для оптимізації моделей, особливо трансформерів, для конкретних обчислювальних платформ. Він включає інструменти для квантування, прунінгу (обрізання), та інших технік оптимізації, щоб зробити моделі швидшими та ефективнішими під час виробництва.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig\n\n\n\nMODEL_PATH = \"/kaggle/input/gemma/transformers/7b-it/1\"\n#MODEL_PATH = \"/kaggle/input/gemma/transformers/2b-it/2\"\n#MODEL_PATH = \"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\"\n#MODEL_PATH = \"/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1\"\n#MODEL_PATH = \"/kaggle/input/phi/transformers/2/1\"\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit = True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, model_max_length=4096)\ntokenizer.padding_side = \"left\"\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_PATH,\n    device_map = \"auto\",\n    trust_remote_code = True,\n    quantization_config=quantization_config,\n)\n","metadata":{"papermill":{"duration":109.931476,"end_time":"2024-03-01T09:45:59.104992","exception":false,"start_time":"2024-03-01T09:44:09.173516","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-05T06:26:56.929969Z","iopub.execute_input":"2024-04-05T06:26:56.930477Z","iopub.status.idle":"2024-04-05T06:29:10.747861Z","shell.execute_reply.started":"2024-04-05T06:26:56.930442Z","shell.execute_reply":"2024-04-05T06:29:10.747041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Імпорти:\n\ntorch: основний пакет для роботи з тензорами і нейронними мережами в Python.\nAutoModelForCausalLM: клас з бібліотеки transformers для автоматичного завантаження моделей, що генерують текст (каузальних мовних моделей).\nAutoTokenizer: клас для автоматичного завантаження токенізатора, який перетворює текст у формат, придатний для моделі.\nBitsAndBytesConfig, AutoConfig: класи для налаштування конфігурації моделі, включаючи параметри квантування.\n## Вказання шляху до моделі:\nMODEL_PATH: змінна, що містить шлях до папки, де збережено переднавчену модель. Коментовані рядки показують альтернативні шляхи, які можуть бути використані.\n## Конфігурація квантування:\nquantization_config: створює конфігурацію для квантування моделі до 4-бітової точності, використовуючи специфічні параметри з бібліотеки bitsandbytes. Це зменшує вимоги до пам'яті і може прискорити виконання моделі, зберігаючи при цьому достатню точність.\n\n## Ініціалізація токенізатора:\ntokenizer: завантажує токенізатор для моделі з вказаного шляху і налаштовує максимальну довжину вхідних даних (4096 символів). Також налаштовується токенізатор на використання лівого вирівнювання для доповнення токенів.\n\n## Ініціалізація моделі:\nmodel: завантажує модель для каузального мовного моделювання з вказаного шляху, використовуючи автоматичне визначення пристрою (GPU, TPU тощо) та приймаючи конфігурацію квантування. Опція trust_remote_code дозволяє завантажувати та використовувати код зі вказаного джерела, що може бути потрібним для коректної роботи деяких моделей.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom string import Template\nfrom pathlib import Path\nimport numpy as np\nimport os\n\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\nimport torch\nfrom transformers import pipeline, AutoTokenizer\n\nfrom tqdm.notebook import tqdm\n\ndata_path = Path('/kaggle/input/llm-prompt-recovery')\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    test = pd.read_csv(data_path / 'test.csv', index_col='id')\n    test[\"rewrite_prompt\"] = \"-\"\nelse:\n    test = pd.read_csv(data_path / 'train.csv', index_col='id')\ntest.head()","metadata":{"papermill":{"duration":12.816958,"end_time":"2024-03-01T09:46:11.926766","exception":false,"start_time":"2024-03-01T09:45:59.109808","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-05T06:29:10.748983Z","iopub.execute_input":"2024-04-05T06:29:10.749291Z","iopub.status.idle":"2024-04-05T06:29:22.894656Z","shell.execute_reply.started":"2024-04-05T06:29:10.749266Z","shell.execute_reply":"2024-04-05T06:29:22.893473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Імпорт функції pipeline та класу AutoTokenizer з бібліотеки transformers для використання моделей обробки природної мови.\nІмпорт tqdm з модулю tqdm.notebook для відображення інтерактивних прогрес-барів у Jupyter Notebook.\n\n## Зчитування даних:\n\ndata_path: змінна, яка визначає шлях до даних, збережених у форматі, зручному для Kaggle.\nПеревірка змінної оточення KAGGLE_IS_COMPETITION_RERUN для визначення, чи перезапущено конкурс Kaggle. Це важливо для того, щоб розуміти, які дані використовувати (тестові чи тренувальні).\nЗалежно від умови, зчитуються відповідні файли CSV (тестові чи тренувальні) з використанням pandas.read_csv із зазначенням індексу.\n## Відображення даних:\n\ntest.head(): показує перші п'ять рядків датафрейму, який містить тестові або тренувальні дані.","metadata":{}},{"cell_type":"code","source":"from torch import nn\nclass Perplexity(nn.Module):\n    def __init__(self, reduce: bool = True):\n        super().__init__()\n        self.loss_fn = nn.CrossEntropyLoss()\n        self.reduce = reduce\n\n    def forward(self, logits, labels):\n        shift_logits = logits[..., :-1, :].contiguous()\n        shift_labels = labels[..., 1:].contiguous()\n\n        perplexity = []\n        for i in range(labels.shape[0]):\n            perplexity.append(self.loss_fn(shift_logits[i], shift_labels[i]))\n        perplexity = torch.stack(perplexity, dim=0)\n        #perplexity = torch.exp(perplexity)\n        if self.reduce:\n            perplexity = torch.mean(perplexity)\n        return perplexity \n    \nperp = Perplexity()","metadata":{"papermill":{"duration":0.015272,"end_time":"2024-03-01T09:46:11.947006","exception":false,"start_time":"2024-03-01T09:46:11.931734","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-05T06:29:22.896032Z","iopub.execute_input":"2024-04-05T06:29:22.896422Z","iopub.status.idle":"2024-04-05T06:29:22.906979Z","shell.execute_reply.started":"2024-04-05T06:29:22.896388Z","shell.execute_reply":"2024-04-05T06:29:22.905778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Цей код визначає клас Perplexity, який є підкласом torch.nn.Module із бібліотеки PyTorch. Цей клас призначений для розрахунку перплексії, яка є метрикою для оцінки якості моделей машинного навчання, зокрема, моделей, які генерують послідовності (наприклад, мовні моделі). Ось докладний опис компонентів цього класу:\n\n## __init__\nreduce: Цей параметр визначає, чи потрібно зводити масив значень перплексії до одного середнього значення. Якщо True, то перплексія буде середнім значенням по всім прикладам у батчі; якщо False, то перплексія повертається для кожного окремого прикладу.\nloss_fn: Використовується функція втрат nn.CrossEntropyLoss, яка є стандартною для класифікації з м'якими мітками (soft labels) та є зручною для обчислення перплексії.\n## forward\nПриймає два аргументи: logits (логіти виходу моделі) та labels (істинні мітки класів).\nshift_logits та shift_labels: Зміщує тензори, щоб відповідні мітки відповідали виходам моделі, зміщеним на один крок в часі. Це зміщення необхідне для правильного обчислення перплексії, де прогноз для кожного кроку часу порівнюється з наступною міткою у послідовності.\nВнутрішній цикл ітерує через всі приклади у батчі, обчислюючи перплексію для кожного з них окремо.\ntorch.stack: Об'єднує масив значень перплексії у тензор.\nКоментований рядок torch.exp(perplexity): Зазвичай, для перплексії потрібно взяти експоненту від середнього значення крос-ентропії, щоб перевести її у більш інтуїтивно зрозумілу метрику. Однак, в цьому прикладі цей рядок закоментовано, що може бути помилкою або свідомим вибором для повернення сирової крос-ентропії.\nУмовний оператор перевіряє, чи потрібно зменшити розмірність результату до одного середнього значення.","metadata":{}},{"cell_type":"code","source":"def format_prompt(row, prompt):\n    prompt_ = f\"\"\"<start_of_turn>user\n{prompt}\n{row[\"original_text\"]}<end_of_turn>\n<start_of_turn>model\n{row[\"rewritten_text\"]}<end_of_turn>\"\"\"\n    return prompt_","metadata":{"papermill":{"duration":0.011681,"end_time":"2024-03-01T09:46:11.963396","exception":false,"start_time":"2024-03-01T09:46:11.951715","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-05T06:29:22.908195Z","iopub.execute_input":"2024-04-05T06:29:22.908541Z","iopub.status.idle":"2024-04-05T06:29:22.922788Z","shell.execute_reply.started":"2024-04-05T06:29:22.908511Z","shell.execute_reply":"2024-04-05T06:29:22.921902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rewrite_prompts = [\n    'Please improve this text using the writing style with maintaining the original meaning but altering the tone.',\n]","metadata":{"papermill":{"duration":0.011349,"end_time":"2024-03-01T09:46:11.979425","exception":false,"start_time":"2024-03-01T09:46:11.968076","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-05T06:29:22.923972Z","iopub.execute_input":"2024-04-05T06:29:22.924381Z","iopub.status.idle":"2024-04-05T06:29:22.931112Z","shell.execute_reply.started":"2024-04-05T06:29:22.924348Z","shell.execute_reply":"2024-04-05T06:29:22.930136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\n\nfor idx, row in tqdm(test.iterrows(), total=len(test)):\n        \n    \n    with torch.no_grad():\n        perps = []\n        samples = []\n        for prompt in rewrite_prompts:\n            samples.append(format_prompt(row, prompt))\n        inputs = tokenizer(samples, return_tensors=\"pt\", add_special_tokens=False, padding=True, truncation=True).to(\"cuda\")\n\n        output = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n        output = output.logits\n        labels = inputs[\"input_ids\"]\n        labels.masked_fill_(~inputs[\"attention_mask\"].bool(), -100)\n        for j in range(len(rewrite_prompts)):\n            p = perp(output[j].unsqueeze(0), labels[j].unsqueeze(0))\n            perps.append(p.detach().cpu())\n            \n        del inputs\n        del labels\n        del output\n        del p\n\n    perps = np.array(perps)\n        \n    predictions = [np.array(rewrite_prompts)[np.argsort(perps)][0]]\n    preds.append(predictions[0])\n    print(preds)","metadata":{"papermill":{"duration":3.907872,"end_time":"2024-03-01T09:46:15.892143","exception":false,"start_time":"2024-03-01T09:46:11.984271","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-05T06:29:22.934422Z","iopub.execute_input":"2024-04-05T06:29:22.934751Z","iopub.status.idle":"2024-04-05T06:29:24.741293Z","shell.execute_reply.started":"2024-04-05T06:29:22.934715Z","shell.execute_reply":"2024-04-05T06:29:24.740179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Цей фрагмент коду на Python призначений для оцінювання серії підказок шляхом вимірювання їхньої перплексії за допомогою мовної моделі, а потім вибору підказки з найнижчою перплексією для кожного прикладу в наборі даних. Цей процес зазвичай використовується у сценаріях, коли ви хочете знайти найбільш \"природну\" або \"ймовірну\" переробку даного тексту на основі прогнозів моделі.\n\n## Основні змінні та налаштування\npreds: Список для зберігання вибраної підказки з найнижчою перплексією для кожного рядка в наборі даних.\ntest: DataFrame, який, ймовірно, містить текстові дані для обробки.\nrewrite_prompts: Список альтернативних підказок для тестування кожного рядка в наборі даних.\nЦикл через дані\nІтерація через test:\nfor idx, row in tqdm(test.iterrows(), total=len(test)): Ітерує через кожен рядок у DataFrame test. Використовує tqdm для прогрес-бару, що вказує, як далеко просунувся процес.\nГенерація та оцінка підказок\nДля кожного рядка:\n## Генерація зразків:\n\nsamples = []: Тимчасовий список для зберігання відформатованих підказок.\nfor prompt in rewrite_prompts: Проходить через кожну попередньо визначену підказку.\nsamples.append(format_prompt(row, prompt)): Викликає format_prompt, щоб відформатувати кожну підказку з поточними даними рядка і додає її до samples.\n## Токенізація та підготовка вхідних даних:\n\ninputs = tokenizer(...): Використовує токенізатор для перетворення текстових зразків у формат, придатний для моделі, включно з налаштуванням на відповідний пристрій (cuda).\n## Виведення моделі:\n\noutput = model(...): Надсилає токенізовані вхідні дані до моделі, щоб отримати логіти.\nlabels = inputs[\"input_ids\"]: Встановлює мітки для розрахунку перплексії (зазвичай мітки - зсунуті ID вхідних даних для мовних моделей).\nlabels.masked_fill_(...): Маскує мітки, де маски уваги є хибними, використовуючи -100 для ігнорування цих у розрахунку втрат.\n## Розрахунок перплексії:\n\nperps = []: Список для зберігання розрахованих перплексій для кожної підказки.\nfor j in range(len(rewrite_prompts)): Ітерує через кожну відповідь від моделі.\np = perp(...): Викликає екземпляр класу Perplexity, щоб обчислити перплексію для кожної підказки.\nperps.append(p.detach().cpu()): Відокремлює перплексію від GPU, переміщує на CPU і додає до списку.\nОчищення та вибір\nОчищення:\n\ndel inputs, labels, output, p: Явно видаляє тензори, щоб звільнити пам'ять GPU.\nВибір найкращої підказки:\n\nperps = np.array(perps): Перетворює список перплексій у масив numpy.\npredictions = [np.array(rewrite_prompts)[np.argsort(perps)][0]]: Знаходить індекс найменшої перплексії, отримує відповідну підказку та зберігає її.\npreds.append(predictions[0]): Додає кращу підказку до кінцевого списку.\nprint(preds): Опціонально виводить список кращих підказок для відстеження прогресу або для відлагодження.","metadata":{}},{"cell_type":"code","source":"submission = pd.read_csv(data_path / 'sample_submission.csv')\nsubmission[\"rewrite_prompt\"] = preds","metadata":{"papermill":{"duration":0.01536,"end_time":"2024-03-01T09:46:15.912885","exception":false,"start_time":"2024-03-01T09:46:15.897525","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-05T06:29:24.742935Z","iopub.execute_input":"2024-04-05T06:29:24.743341Z","iopub.status.idle":"2024-04-05T06:29:24.755346Z","shell.execute_reply.started":"2024-04-05T06:29:24.743309Z","shell.execute_reply":"2024-04-05T06:29:24.754047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"papermill":{"duration":0.014739,"end_time":"2024-03-01T09:46:15.953356","exception":false,"start_time":"2024-03-01T09:46:15.938617","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-05T06:29:24.757012Z","iopub.execute_input":"2024-04-05T06:29:24.757371Z","iopub.status.idle":"2024-04-05T06:29:24.76485Z","shell.execute_reply.started":"2024-04-05T06:29:24.757341Z","shell.execute_reply":"2024-04-05T06:29:24.763853Z"},"trusted":true},"execution_count":null,"outputs":[]}]}